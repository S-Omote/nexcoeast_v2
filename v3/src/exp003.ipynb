{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Library\n",
    "# ========================================\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import jpholiday\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "from scipy.optimize import minimize\n",
    "import lightgbm as lgb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_df = pd.read_csv('C:/Users/brain/Desktop/nexcoeast_v2/train/road.csv')\n",
    "search_spec_df = pd.read_csv('C:/Users/brain/Desktop/nexcoeast_v2/train/search_data.csv')\n",
    "search_unspec_df = pd.read_csv('C:/Users/brain/Desktop/nexcoeast_v2/train/search_unspec_data.csv')\n",
    "train_df = pd.read_csv('C:/Users/brain/Desktop/nexcoeast_v2/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_datetime(df):\n",
    "    if 'datetime' in df.columns:\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "    if 'date' in df.columns:\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day'] = df['date'].dt.day\n",
    "\n",
    "    return df\n",
    "\n",
    "# 条件分岐の関数を定義\n",
    "def judge_dayoff(x):\n",
    "    # 土日なら1を返す\n",
    "    if  x.is_holiday == 1 or x.dayofweek > 4:\n",
    "        return 1\n",
    "    # 平日なら0を返す\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(train_df, search_spec_df, search_unspec_df):\n",
    "    train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "    search_spec_df['datetime'] = pd.to_datetime(search_spec_df['datetime'])\n",
    "    search_unspec_df['date'] = pd.to_datetime(search_unspec_df['date'])\n",
    "    # dateから、祝日か否か判定する。祝日なら1。\n",
    "    search_unspec_df['is_holiday'] = search_unspec_df['date'].map(jpholiday.is_holiday).astype(int)\n",
    "\n",
    "    train_df = expand_datetime(train_df)\n",
    "    # search_spec_df = expand_datetime(search_spec_df)\n",
    "    search_unspec_df = expand_datetime(search_unspec_df)\n",
    "\n",
    "    train_df = train_df.merge(search_spec_df, on=['datetime', 'start_code', 'end_code'], how='left')\n",
    "    train_df = train_df.merge(search_unspec_df, on=['year', 'month', 'day', 'start_code', 'end_code'], how='left')\n",
    "    train_df = train_df.merge(road_df.drop(['start_name', 'end_name'], axis=1), on=['start_code', 'end_code'], how='left')\n",
    "\n",
    "    train_df['dayofweek'] = train_df['datetime'].dt.weekday\n",
    "\n",
    "    # 休日と平日の判定\n",
    "    train_df['is_dayoff'] = train_df.apply(lambda x:judge_dayoff(x), axis=1)\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = extract_dataset(train_df, search_spec_df, search_unspec_df)\n",
    "\n",
    "train['section'] = train['start_code'].astype(str)+'_'+train['end_code'].astype(str)\n",
    "\n",
    "cat_cols = ['road_code', 'start_code', 'end_code', 'section', 'direction', 'dayofweek', 'is_holiday', 'is_dayoff']\n",
    "num_cols = ['year', 'month', 'day', 'hour', 'search_1h', 'search_unspec_1d', 'KP', 'start_KP', 'end_KP', 'limit_speed',\n",
    "            'OCC', 'allCars', 'speed', 'start_pref_code', 'end_pref_code', 'start_lat', 'end_lat', 'start_lng', 'end_lng',\n",
    "            'start_degree', 'end_degree']\n",
    "feature_cols = cat_cols + num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# カテゴリ変数の処理\n",
    "# ========================================\n",
    "le_dict = {}\n",
    "for c in tqdm(cat_cols):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[c])\n",
    "    train[c] = le.transform(train[c])\n",
    "    le_dict[c] = le\n",
    "\n",
    "with open(\"features/le_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# training module\n",
    "# ========================================\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    score = f1_score(y_true, y_pred)\n",
    "    return 'f1', score, True\n",
    "\n",
    "def train_lgbm(X,\n",
    "               y,\n",
    "               cv,\n",
    "               model_path = [],\n",
    "               params: dict=None,\n",
    "               verbose: int=100\n",
    "               ):\n",
    "\n",
    "    # パラメータがないときは、空の dict で置き換える\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    models = []\n",
    "    n_records = len(X)\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    oof_pred = np.zeros((n_records, ), dtype=np.float32)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv):\n",
    "        x_train, y_train = X[idx_train], y[idx_train]\n",
    "        x_valid, y_valid = X[idx_valid], y[idx_valid]\n",
    "\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params)\n",
    "\n",
    "        clf.fit(x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=verbose,\n",
    "                eval_metric=f1,\n",
    "                )\n",
    "\n",
    "        pred_i = clf.predict(x_valid)\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(clf)\n",
    "        score = f1_score(y_valid, (pred_i > 0.5).astype(int))\n",
    "        print(f\" - fold{i + 1} - {score:.4f}\")\n",
    "\n",
    "    score = f1_score(y, (oof_pred > 0.5).astype(int))\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"FINISH: CV Score: {score:.4f}\")\n",
    "    return score, oof_pred, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# parameters\n",
    "# ========================================\n",
    "max_depth = 7\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"max_depth\": max_depth,\n",
    "    'num_leaves' : int(0.7 * 2 ** max_depth),#240\n",
    "    \"n_estimators\": 100000,\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"importance_type\": \"gain\",\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "# ========================================\n",
    "# train-validation split\n",
    "# ========================================\n",
    "TARGET = 'is_congestion'\n",
    "N_SPLIT = 5\n",
    "kf = StratifiedGroupKFold(N_SPLIT)\n",
    "cv_list = list(kf.split(train, y=train[TARGET], groups=train['date']))\n",
    "\n",
    "# ========================================\n",
    "# define variables\n",
    "# ========================================\n",
    "X = train[feature_cols].values\n",
    "y = train[TARGET].values\n",
    "\n",
    "print('train shape:', train.shape)\n",
    "# ========================================\n",
    "# training\n",
    "# ========================================\n",
    "score, oof_pred, models = train_lgbm(X, y=y, params=params, cv=cv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(models):\n",
    "   with open(f'../model/lgb_fold{i}.pickle', mode=\"wb\") as f:\n",
    "      pickle.dump(m, f)\n",
    "\n",
    "print(classification_report(train['is_congestion'], oof_pred))\n",
    "print(train['is_congestion'].sum(), oof_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# feature importance\n",
    "# ========================================\n",
    "def visualize_importance(models, feat_train_df):\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df[\"feature_importance\"] = model.feature_importances_\n",
    "        _df[\"column\"] = feat_train_df.columns\n",
    "        _df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, _df],\n",
    "                                          axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance_df.groupby(\"column\")\\\n",
    "        .sum()[[\"feature_importance\"]]\\\n",
    "        .sort_values(\"feature_importance\", ascending=False).index\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, len(order) * .25)))\n",
    "    sns.boxplot(data=feature_importance_df,\n",
    "                  x=\"feature_importance\",\n",
    "                  y=\"column\",\n",
    "                  order=order,\n",
    "                  ax=ax,\n",
    "                  palette=\"viridis\",\n",
    "                  orient=\"h\")\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    ax.set_title(\"Importance\")\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    return fig, ax, feature_importance_df\n",
    "\n",
    "fig, ax, feature_importance_df = visualize_importance(models, train[feature_cols])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
